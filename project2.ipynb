{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74aab069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb493c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faa92341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a01d20a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----+-------+----------+----------------------+-----------------------+------+--------+-----------------+-----+-------+----+\n",
      "|year|month|day|order|country|session ID|page 1 (main category)|page 2 (clothing model)|colour|location|model photography|price|price 2|page|\n",
      "+----+-----+---+-----+-------+----------+----------------------+-----------------------+------+--------+-----------------+-----+-------+----+\n",
      "|2008|    4|  1|    1|     29|         1|                     1|                    A13|     1|       5|                1|   28|      2|   1|\n",
      "|2008|    4|  1|    2|     29|         1|                     1|                    A16|     1|       6|                1|   33|      2|   1|\n",
      "|2008|    4|  1|    3|     29|         1|                     2|                     B4|    10|       2|                1|   52|      1|   1|\n",
      "|2008|    4|  1|    4|     29|         1|                     2|                    B17|     6|       6|                2|   38|      2|   1|\n",
      "|2008|    4|  1|    5|     29|         1|                     2|                     B8|     4|       3|                2|   52|      1|   1|\n",
      "+----+-----+---+-----+-------+----------+----------------------+-----------------------+------+--------+-----------------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path=\"file:///home/talentum/shared/data/e-shop_clothing_2008.csv\"\n",
    "#Loading Input File\n",
    "inputDF=spark.read.csv(file_path,header=True,inferSchema=True)\n",
    "inputDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "935fde8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'order',\n",
       " 'country',\n",
       " 'session_ID',\n",
       " 'Product_Category',\n",
       " 'Clothing_Model',\n",
       " 'colour',\n",
       " 'location',\n",
       " 'model_photography',\n",
       " 'price',\n",
       " 'price_2',\n",
       " 'page']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputDF=inputDF.withColumnRenamed('page 1 (main category)','Product_Category')\n",
    "inputDF=inputDF.withColumnRenamed('page 2 (clothing model)','Clothing_Model')\n",
    "inputDF=inputDF.withColumnRenamed('session ID','session_ID')\n",
    "inputDF=inputDF.withColumnRenamed('model photography','model_photography')\n",
    "inputDF=inputDF.withColumnRenamed('price 2','price_2')\n",
    "\n",
    "inputDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da6abdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year  :  1\n",
      "month  :  5\n",
      "day  :  31\n",
      "order  :  195\n",
      "country  :  47\n",
      "session_ID  :  24026\n",
      "Product_Category  :  4\n",
      "Clothing_Model  :  217\n",
      "colour  :  14\n",
      "location  :  6\n",
      "model_photography  :  2\n",
      "price  :  20\n",
      "price_2  :  2\n",
      "page  :  5\n"
     ]
    }
   ],
   "source": [
    "for column in inputDF.columns:\n",
    "    print(column,\" : \",inputDF.select(column).distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aca597c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysing Features\n",
    "#year,month,day -> No use as it only contain value of smaller duration\n",
    "#order -> USeful \n",
    "#Country -> Useful (main category for classification)\n",
    "#session ID -> Useful\n",
    "# Product_Category,Clothing_Model,colour,location,model photography,price(dollar),price2,page -> Useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cdb65ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----------+----------------+--------------+------+--------+-----------------+-----+-------+----+\n",
      "|order|country|session_ID|Product_Category|Clothing_Model|colour|location|model_photography|price|price_2|page|\n",
      "+-----+-------+----------+----------------+--------------+------+--------+-----------------+-----+-------+----+\n",
      "|    1|     29|         1|               1|           A13|     1|       5|                1|   28|      2|   1|\n",
      "|    2|     29|         1|               1|           A16|     1|       6|                1|   33|      2|   1|\n",
      "|    3|     29|         1|               2|            B4|    10|       2|                1|   52|      1|   1|\n",
      "|    4|     29|         1|               2|           B17|     6|       6|                2|   38|      2|   1|\n",
      "|    5|     29|         1|               2|            B8|     4|       3|                2|   52|      1|   1|\n",
      "+-----+-------+----------+----------------+--------------+------+--------+-----------------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Droping the columns \n",
    "inputDF1=inputDF.drop('year','month','day')\n",
    "inputDF1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d14b4947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order : 0\n",
      "country : 0\n",
      "session_ID : 0\n",
      "Product_Category : 0\n",
      "Clothing_Model : 0\n",
      "colour : 0\n",
      "location : 0\n",
      "model_photography : 0\n",
      "price : 0\n",
      "price_2 : 0\n",
      "page : 0\n"
     ]
    }
   ],
   "source": [
    "#Checking for NULL values\n",
    "for column in inputDF1.columns:\n",
    "    print(column,\":\",inputDF1.filter(col(column).isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a114b0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output dataframe is:\n",
      "+-----+-------+----------+----------------+------+--------+-----------------+-----+-------+----+--------------+\n",
      "|order|country|session_ID|Product_Category|colour|location|model_photography|price|price_2|page|Clothing_Model|\n",
      "+-----+-------+----------+----------------+------+--------+-----------------+-----+-------+----+--------------+\n",
      "|    1|     29|         1|               1|     1|       5|                1|   28|      2|   1|          19.0|\n",
      "|    2|     29|         1|               1|     1|       6|                1|   33|      2|   1|          34.0|\n",
      "|    3|     29|         1|               2|    10|       2|                1|   52|      1|   1|           0.0|\n",
      "|    4|     29|         1|               2|     6|       6|                2|   38|      2|   1|          28.0|\n",
      "|    5|     29|         1|               2|     4|       3|                2|   52|      1|   1|         121.0|\n",
      "+-----+-------+----------+----------------+------+--------+-----------------+-----+-------+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Label encoding on col= Clothing_Model\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"Clothing_Model\", outputCol=\"Clothing_Model_label\") \n",
    "indexed_df = indexer.fit(inputDF1).transform(inputDF1)\n",
    "indexed_df=indexed_df.drop(\"Clothing_Model\").withColumnRenamed(\"Clothing_Model_label\",\"Clothing_Model\")\n",
    "print(\"The output dataframe is:\")\n",
    "indexed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fedd5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, exp\n",
    "def iqr_outlier_treatment(dataframe, columns, factor=1.5):\n",
    "    for column in columns:\n",
    "        # Calculate Q1, Q3, and IQR\n",
    "        quantiles = dataframe.approxQuantile(column, [0.25, 0.75],0.01)\n",
    "        q1, q3 = quantiles[0], quantiles[1]\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        # Define the upper and lower bounds for outliers\n",
    "        lower_bound = q1 - factor * iqr\n",
    "        upper_bound = q3 + factor * iqr\n",
    "\n",
    "        # Filter outliers and update the DataFrame\n",
    "        dataframe = dataframe.filter((col(column) >= lower_bound) & (col(column) <= upper_bound))\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bf9aae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----------+----------------+------+--------+-----------------+-----+-------+----+--------------+\n",
      "|order|country|session_ID|Product_Category|colour|location|model_photography|price|price_2|page|Clothing_Model|\n",
      "+-----+-------+----------+----------------+------+--------+-----------------+-----+-------+----+--------------+\n",
      "|    1|     29|         1|               1|     1|       5|                1|   28|      2|   1|          19.0|\n",
      "|    2|     29|         1|               1|     1|       6|                1|   33|      2|   1|          34.0|\n",
      "|    3|     29|         1|               2|    10|       2|                1|   52|      1|   1|           0.0|\n",
      "|    4|     29|         1|               2|     6|       6|                2|   38|      2|   1|          28.0|\n",
      "|    5|     29|         1|               2|     4|       3|                2|   52|      1|   1|         121.0|\n",
      "+-----+-------+----------+----------------+------+--------+-----------------+-----+-------+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Continuous_cols = ['order','price'] #order and price are only continuous columns\n",
    "df_outlier_treatment = iqr_outlier_treatment(indexed_df, Continuous_cols, factor=1.5)\n",
    "df_outlier_treatment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6451cb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152109"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_outlier_treatment.count() \n",
    "#13,365 rows dropped as result of outlier detection in order and price columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efd7cf7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['session_ID',\n",
       " 'order',\n",
       " 'country',\n",
       " 'Product_Category',\n",
       " 'Clothing_Model',\n",
       " 'colour',\n",
       " 'location',\n",
       " 'model_photography',\n",
       " 'price',\n",
       " 'price_2',\n",
       " 'page',\n",
       " 'count']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#User-Session-level data\n",
    "inputDF2=inputDF1.groupBy('session_ID').count()\n",
    "#Data Engineering\n",
    "df_inner = inputDF1.join(inputDF2, on='session_ID', how='inner')\n",
    "df_inner.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f5b534c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_inner = df_inner.coalesce(1)\n",
    "df_inner.write.csv(\"file:///home/talentum/preprocessed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43372a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
